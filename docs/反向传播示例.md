## 反向传播示例

### 一、 单层网络的梯度下降和反向传播

#### 单因子线性模型

1. 线性模型定义

$$
\hat{y}_i=ax_i+b
$$

2. 损失函数
   $$
   Loss = \frac{1}{2}\sum_i^n(\hat{y}_i-y_i)^2=\frac{1}{2}\sum_i^n(ax_i+b-y_i)^2
   $$

3. 损失函数求偏导(偏导代入a的值即为梯度)

$$
\frac{\partial Loss}{\partial a} =\frac{\partial \frac{1}{2}\sum_i^n(ax_i+b-y_i)^2}{\partial a}=\sum_i^n(ax_i+b-y_i)x_i \\
\frac{\partial Loss}{\partial b} =\frac{\partial \frac{1}{2}\sum_i^n(ax_i+b-y_i)^2}{\partial b}=\sum_i^n(ax_i+b-y_i)
$$

4. 更新参数

   定义学习率lr，代入梯度值，得到a更新后的值
   $$
   a=a-lr*\frac{\partial Loss}{\partial a}\\
   $$
   
5. 代码示例

   1.  构建数据集

      ```python
      import matplotlib.pyplot as plt
      import torch
      from torch.utils.data import TensorDataset, DataLoader
      
      x = torch.arange(1, 100, 2)
      noise = torch.randn(50)
      y = x * 2 + 10
      # y = y + noise
      
      t_data_set = TensorDataset(x, y)
      
      dl = DataLoader(t_data_set, batch_size=5)
      
      a = torch.tensor(20.0, requires_grad=True)
      b = torch.tensor(30.0, requires_grad=True)
      ```

   2. epoch 循环

      ```python
      for epoch in range(100):
          all_loss = 0
          for xt, yt in dl:
              # 损失函数
              y_pred = a * xt + b
              loss = torch.sum((y_pred - yt) ** 2) / 2
              all_loss += loss.data
              # 梯度归零
              if a.grad:
                  a.grad.data.zero_()
                  b.grad.data.zero_()
              # 反向传播      
              loss.backward()
              # 更新数据
              a.data = a.data - a.grad.data * 1e-4
              b.data = b.data - b.grad.data * 1e-3
      ```

      

### 多层网络反向传播

#### 单因子多层网络梯度下降和反向传播

1. 多层线性模型定义
   $$
   复合函数：\hat{y}_i=a_2(a_1x_i+b_1)+b_2\\
   第一层：g(x_i) = a_1x_i+b_1\\
   第二层：f(x_i) = a_2g(x_i)+b_2\\
   需要注意的是，在这里需要把x_i理解为常量，即g(x_1)和g(x_2)是针对于a_1和b_1的不同的方程
   $$

2. 损失函数
   $$
   Loss = \frac{1}{2}\sum_i^n(\hat{y}_i-y_i)^2=\frac{1}{2}\sum_i^n(a_2g(x_i)+b_2-y_i)^2=\frac{1}{2}\sum_i^n(a_2(a_1x_i+b_1)+b_2-y_i)^2
   $$

3. 先对a2、b2求导
   $$
   \frac{\partial Loss}{\partial a_1} =\frac{\partial \frac{1}{2}\sum_i^n(a_2g(x_i)+b_2-y_i)^2}{\partial a_2}=\sum_i^n(a_2g(x_i)+b_2-y_i)g(x_i) \\
   \frac{\partial Loss}{\partial b} =\frac{\partial \frac{1}{2}\sum_i^n(a_2g(x_i)+b_2-y_i)^2}{\partial b_2}=\sum_i^n(a_2g(x_i)+b_2-y_i)
   $$

4. 链式法则说明
   $$
   若 y=f(u), u=g(x)\\
   则\frac{\part y}{\part x}=\frac{\part y}{\part u}\frac{\part u}{\part x}
   $$

5. 基于链式法则对a1、b1求导
   $$
   Loss = \frac{1}{2}\sum_i^n(f(g(x_i))-y_i)^2\\
   \frac{\part Loss}{\part a_1}=\sum_i^n[(f(g(x_i))-y_i)\frac{\part f(g(x_i))}{\part g(x_i)}\frac{\part g(xi)}{\part a_1}]=\sum_i^n(f(g(x_i))-y_i)a_2x_i\\
   \frac{\part Loss}{\part b_1}=\sum_i^n[(f(g(x_i))-y_i)\frac{\part f(g(x_i))}{\part g(x_i)}\frac{\part g(xi)}{\part b_1}]=\sum_i^n(f(g(x_i))-y_i)a_2\\
   $$

6. 代码示例

   ```python
   import seaborn as sns
   import matplotlib.pyplot as plt
   import torch
   from torch.utils.data import TensorDataset, DataLoader
   import pandas as pd
   
   x = torch.arange(1, 100, 2)
   noise = torch.randn(50)
   y = x * 2 + 10
   # y = y + noise
   
   t_data_set = TensorDataset(x, y)
   
   dl = DataLoader(t_data_set, batch_size=5)
   
   # 两层神经网络
   a1 = torch.tensor(20.0, requires_grad=True)
   b1 = torch.tensor(30.0, requires_grad=True)
   
   a2 = torch.tensor(20.0, requires_grad=True)
   b2 = torch.tensor(30.0, requires_grad=True)
   flag = 0
   for epoch in range(1):
       all_loss = 0
       for xt, yt in dl:
           # 损失函数
           liner1 = a1 * xt + b1
           y_pred = a2 * liner1 + b2
           loss = torch.sum((y_pred - yt) ** 2) / 2
           all_loss += loss.data
           # 梯度归零
           if flag != 0:
               a1.grad.data.zero_()
               b1.grad.data.zero_()
               a2.grad.data.zero_()
               b2.grad.data.zero_()
           else:
               flag = 1
           loss.backward()
           print(f"a1:{a1.data}, a2:{a2.data}, b1:{b1.data},b2:{b2.data}")
           print(f"自动计算的梯度：a1_grad:{a1.grad}, a2_grad:{a2.grad}, b1_grad:{b1.grad},b_grad:{b2.grad}")
           print(f"x: {xt}, y: {yt}, 第一层结果：{liner1}, 第二层结果：{y_pred}")
           print(f"计算对a2梯度：{torch.sum(torch.mul(y_pred - yt, liner1))}")
           print(f"loss对liner1的梯度：{a2 * (y_pred - yt)}")
           print(f"liner1对a1的梯度：{xt}")
           print(f"loss对a1的梯度：{torch.sum(torch.mul(xt, a2 * (y_pred - yt)))}")
           print(f"loss对b1的梯度：{torch.sum(a2 * (y_pred - yt))}")
   
           # 更新数据
           a1.data = a1.data - a1.grad.data * 1e-4
           b1.data = b1.data - b1.grad.data * 1e-3
           a2.data = a2.data - a2.grad.data * 1e-4
           b2.data = b2.data - b2.grad.data * 1e-3
           break
       print(f"epoch:{epoch}, now a:{a1}, now b:{b1}, now loss: {all_loss / len(dl)}")
   
   # y_pre = a * x + b
   # plt.plot(x.detach().numpy(), y.detach().numpy(), 'go', label='data', alpha=0.3)
   # plt.plot(x.detach().numpy(), y_pre.detach().numpy(),
   #          label='predicted', alpha=1)
   # plt.legend()
   # plt.show()
   ```

   7. pytorch 简单写法:
   
      ```python
      import matplotlib.pyplot as plt
      import torch
      from torch import nn
      from torch.optim import SGD, Adam
      from torch.utils.data import TensorDataset, DataLoader
      
      x = torch.arange(1, 100, 2, dtype=torch.float32)
      noise = torch.randn(50)
      y = x * 2 + 10
      y = y + noise * 20
      t_data_set = TensorDataset(x, y)
      dl = DataLoader(t_data_set, batch_size=5)
      
      model = nn.Sequential(nn.Linear(1, 1), nn.Linear(1, 1))
      criterion = nn.MSELoss()
      # adam 牛逼
      optim = Adam(model.parameters(), lr=1e-3)
      # optim = Adam(model.parameters(), lr=1e-3)
      
      for epoch in range(1000):
          all_loss = 0
          for xt, yt in dl:
              outputs = model(xt.unsqueeze(1))
              optim.zero_grad()
              loss = criterion(yt, outputs.squeeze())
              loss.backward()
              optim.step()
              all_loss += loss.detach().data
          print(f"epoch:{epoch},  now loss: {all_loss / len(dl)}")
      
      y_pred = model(x.unsqueeze(1)).squeeze()
      plt.plot(x.detach().numpy(), y.detach().numpy(), 'go', label='data', alpha=0.3)
      plt.plot(x.detach().numpy(), y_pred.detach().numpy(), label='predicted', alpha=1)
      plt.legend()
      plt.show()
      ```
   
      

#### 多层网络多因子梯度下降和反向传播

